{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the working directory is set to the \"ma1\" folder.\n",
    "while Path.cwd().name != \"ma2\" and \"ma2\" in str(Path.cwd()): \n",
    "    os.chdir(\"..\")  # Move up one directory\n",
    "print(f\"Working directory set to: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"padding-left: 10px; padding-right: 10px; padding-top: 10px; padding-bottom: 30px, align: justify\">\n",
    "<p align=\"center\">\n",
    "<img src=\"../media/bow_slp1.png\" alt=\"Transformer Architecture\" width=\"800\"/>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "# An introduction to bag-of-words models\n",
    "\n",
    "* A bag-of-words model is a type of vector representation for text data that describes the occurrence of words within a document.<br><br>\n",
    "* The bag-of-words (BoW) model represents one of the most venerable methodologies in Natural Language Processing (NLP), where text is decomposed into a “bag” of its constituent words without explicit regard for syntax or word order.<br><br>\n",
    "* Early conceptual groundwork for this type of representation can be traced back to seminal ideas on textual indexing and term frequency analysis (such as G. Salton's \"<i>Automatic Information Organization and Retrieval</i>\", 1968). Since then, bag-of-words has served as a cornerstone in a broad array of NLP tasks, including document classification, topic modeling, and information retrieval. <br><br>\n",
    "* In essence, bag-of-words transforms each document into a vector of word counts or frequencies. This vectorization enables a range of statistical and machine learning methods — originally developed for structured numerical data—to operate on textual data. While more sophisticated models such as TF-IDF, word embeddings, and transformer-based architectures have become prominent, the conceptual simplicity and interpretability of BoW continue to make it a useful tool in both research and industry to this day.<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "As shown in the image above, we represent a text document bag of words as if it were, well, a bag of words. That is, any sequence of text (or document) becomes an unordered set of words with their position ignored, keeping only their frequency in the document. In the example in the figure, instead of representing the word order in all the phrases like “<i>I love this movie</i>” and “<i>I would recommend it</i>”, we simply note that the word I occurred 5 times in the entire excerpt, the word it 6 times, the words love, recommend, and movie once, and so on. In other words, no pun intended, the position of the words is ignored (the bag-of-words assumption) and we make use of the *frequency* of each word.\n",
    "\n",
    "## A video introduction to bag-of-words models\n",
    "* To get started, please watch this video (**0:46 up to 4:26**): [Getting started with Natural Language Processing: Bag of words](https://youtu.be/UFtXy0KRxVI?t=46) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intuition behind bag-of-words: BoW from scratch\n",
    "\n",
    "In this section, we will build a simple bag-of-words model from scratch. We will\n",
    "1. Build intuition on how to go from a text document to a bag-of-words representation.\n",
    "2. Show the preprocessing steps required to build a bag-of-words model, including lowercasing, tokenization, symbol removal and stop-word removal.\n",
    "3. Implement a simple bag-of-words model in Python and use it to encode a corpus of text documents.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "To build some intuition, let's assume we have the same text as in the image above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"I love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!\"\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to turn that into a bag-of-words representation, we would count the frequency of each word in the text. Without any preprocessing of the text, the result would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 3, 'love': 1, 'this': 1, 'movie!': 1, \"It's\": 1, 'sweet,': 1, 'but': 1, 'with': 1, 'satirical': 1, 'humor.': 1, 'The': 1, 'dialogue': 1, 'is': 1, 'great': 1, 'and': 3, 'the': 3, 'adventure': 1, 'scenes': 1, 'are': 1, 'fun...': 1, 'It': 1, 'manages': 1, 'to': 3, 'be': 1, 'whimsical': 1, 'romantic': 1, 'while': 1, 'laughing': 1, 'at': 1, 'conventions': 1, 'of': 1, 'fairy': 1, 'tale': 1, 'genre.': 1, 'would': 1, 'recommend': 1, 'it': 4, 'just': 1, 'about': 1, 'anyone.': 1, \"I've\": 1, 'seen': 2, 'several': 1, 'times,': 1, \"I'm\": 1, 'always': 1, 'happy': 1, 'see': 1, 'again': 1, 'whenever': 1, 'have': 1, 'a': 1, 'friend': 1, 'who': 1, \"hasn't\": 1, 'yet!': 1}\n"
     ]
    }
   ],
   "source": [
    "# split the document into words\n",
    "words = document.split() # split the document into words by whitespace\n",
    "\n",
    "# create a dictionary to store the frequency of each word\n",
    "word_freq = {}\n",
    "for word in words:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the word \"I\" appears 3 times, \"love\" appears 1 time, \"this\" appears 1 time, and so on. This is the essence of the bag-of-words model. But wait! We also have \"words\" like \"It's\" and \"humor.\" And how about if the words \"The\" and \"the\"? They should count as the same word, right? \n",
    "\n",
    "How do we handle these cases? Well, we need to preprocess the text to ensure that we are counting the words correctly. This is the first step in the bag-of-words model: **text preprocessing**.\n",
    "\n",
    "1. **Lowercasing**: Ensuring that all words are in the same case.\n",
    "2. **Removing punctuation**: Removing any punctuation marks from the text.\n",
    "3. **Tokenization**: Splitting the text into words or phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'this', 'movie', 'its', 'sweet', 'but', 'with', 'satirical', 'humor', 'the', 'dialogue', 'is', 'great', 'and', 'the', 'adventure', 'scenes', 'are', 'fun', 'it', 'manages', 'to', 'be', 'whimsical', 'and', 'romantic', 'while', 'laughing', 'at', 'the', 'conventions', 'of', 'the', 'fairy', 'tale', 'genre', 'i', 'would', 'recommend', 'it', 'to', 'just', 'about', 'anyone', 'ive', 'seen', 'it', 'several', 'times', 'and', 'im', 'always', 'happy', 'to', 'see', 'it', 'again', 'whenever', 'i', 'have', 'a', 'friend', 'who', 'hasnt', 'seen', 'it', 'yet']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "\n",
    "    # 1. Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove punctuation by building a new string without those characters\n",
    "    # Define punctuation characters explicitly (since we are not using any libraries)\n",
    "    punctuation_chars = \".,!?;:'\\\"()\"\n",
    "    text = ''.join(char for char in text if char not in punctuation_chars)\n",
    "\n",
    "    # 3. Tokenize by splitting on whitespace\n",
    "    tokens = text.split()\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Process the document and print results\n",
    "tokens = preprocess_text(document)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build our term frequency dictionary again with our preprocessed text/tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 3, 'love': 1, 'this': 1, 'movie': 1, 'its': 1, 'sweet': 1, 'but': 1, 'with': 1, 'satirical': 1, 'humor': 1, 'the': 4, 'dialogue': 1, 'is': 1, 'great': 1, 'and': 3, 'adventure': 1, 'scenes': 1, 'are': 1, 'fun': 1, 'it': 5, 'manages': 1, 'to': 3, 'be': 1, 'whimsical': 1, 'romantic': 1, 'while': 1, 'laughing': 1, 'at': 1, 'conventions': 1, 'of': 1, 'fairy': 1, 'tale': 1, 'genre': 1, 'would': 1, 'recommend': 1, 'just': 1, 'about': 1, 'anyone': 1, 'ive': 1, 'seen': 2, 'several': 1, 'times': 1, 'im': 1, 'always': 1, 'happy': 1, 'see': 1, 'again': 1, 'whenever': 1, 'have': 1, 'a': 1, 'friend': 1, 'who': 1, 'hasnt': 1, 'yet': 1}\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to store the frequency of each word\n",
    "word_freq = {}\n",
    "for word in tokens:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better! We have successfully built a term frequency dictionary for our text: A registry of the counts of each word in our document.\n",
    "\n",
    "But what if we have multiple texts? How do we build a term frequency dictionary for all of them? A set of document texts is called a **corpus**. Let's define a simple corpus and then build a TF dictionary for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Natural Language Processing (NLP) is a fascinating field of study, which involves the interaction between computers and humans using natural language.\",\n",
    "    \"The goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\",\n",
    "    \"There are many challenges in NLP, such as dealing with the ambiguity and variability of natural language.\",\n",
    "    \"Techniques in NLP include tokenization, stemming, lemmatization, and part-of-speech tagging, among others.\",\n",
    "    \"Applications of NLP are vast and include machine translation, sentiment analysis, and speech recognition.\",\n",
    "    \"In recent years, deep learning has revolutionized NLP, leading to significant improvements in tasks like language modeling and text generation.\",\n",
    "    \"Despite these advancements, there are still many open problems in NLP, such as understanding context and handling low-resource languages.\",\n",
    "    \"Researchers in NLP are continually developing new methods and models to address these challenges and improve the performance of NLP systems.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(preprocessed_documents)=8 | len(corpus)=8\n",
      "\n",
      "[['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'of', 'study', 'which', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language'], ['the', 'goal', 'of', 'nlp', 'is', 'to', 'enable', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'useful'], ['there', 'are', 'many', 'challenges', 'in', 'nlp', 'such', 'as', 'dealing', 'with', 'the', 'ambiguity', 'and', 'variability', 'of', 'natural', 'language'], ['techniques', 'in', 'nlp', 'include', 'tokenization', 'stemming', 'lemmatization', 'and', 'part-of-speech', 'tagging', 'among', 'others'], ['applications', 'of', 'nlp', 'are', 'vast', 'and', 'include', 'machine', 'translation', 'sentiment', 'analysis', 'and', 'speech', 'recognition'], ['in', 'recent', 'years', 'deep', 'learning', 'has', 'revolutionized', 'nlp', 'leading', 'to', 'significant', 'improvements', 'in', 'tasks', 'like', 'language', 'modeling', 'and', 'text', 'generation'], ['despite', 'these', 'advancements', 'there', 'are', 'still', 'many', 'open', 'problems', 'in', 'nlp', 'such', 'as', 'understanding', 'context', 'and', 'handling', 'low-resource', 'languages'], ['researchers', 'in', 'nlp', 'are', 'continually', 'developing', 'new', 'methods', 'and', 'models', 'to', 'address', 'these', 'challenges', 'and', 'improve', 'the', 'performance', 'of', 'nlp', 'systems']]\n",
      "len(word_freq)=140\n",
      "\n",
      "{'i': 3, 'love': 1, 'this': 1, 'movie': 1, 'its': 1, 'sweet': 1, 'but': 1, 'with': 2, 'satirical': 1, 'humor': 1, 'the': 8, 'dialogue': 1, 'is': 4, 'great': 1, 'and': 14, 'adventure': 1, 'scenes': 1, 'are': 5, 'fun': 1, 'it': 5, 'manages': 1, 'to': 7, 'be': 1, 'whimsical': 1, 'romantic': 1, 'while': 1, 'laughing': 1, 'at': 1, 'conventions': 1, 'of': 6, 'fairy': 1, 'tale': 1, 'genre': 1, 'would': 1, 'recommend': 1, 'just': 1, 'about': 1, 'anyone': 1, 'ive': 1, 'seen': 2, 'several': 1, 'times': 1, 'im': 1, 'always': 1, 'happy': 1, 'see': 1, 'again': 1, 'whenever': 1, 'have': 1, 'a': 3, 'friend': 1, 'who': 1, 'hasnt': 1, 'yet': 1, 'natural': 3, 'language': 5, 'processing': 1, 'nlp': 9, 'fascinating': 1, 'field': 1, 'study': 1, 'which': 1, 'involves': 1, 'interaction': 1, 'between': 1, 'computers': 2, 'humans': 1, 'using': 1, 'goal': 1, 'enable': 1, 'understand': 1, 'interpret': 1, 'generate': 1, 'human': 1, 'in': 7, 'way': 1, 'that': 1, 'both': 1, 'meaningful': 1, 'useful': 1, 'there': 2, 'many': 2, 'challenges': 2, 'such': 2, 'as': 2, 'dealing': 1, 'ambiguity': 1, 'variability': 1, 'techniques': 1, 'include': 2, 'tokenization': 1, 'stemming': 1, 'lemmatization': 1, 'part-of-speech': 1, 'tagging': 1, 'among': 1, 'others': 1, 'applications': 1, 'vast': 1, 'machine': 1, 'translation': 1, 'sentiment': 1, 'analysis': 1, 'speech': 1, 'recognition': 1, 'recent': 1, 'years': 1, 'deep': 1, 'learning': 1, 'has': 1, 'revolutionized': 1, 'leading': 1, 'significant': 1, 'improvements': 1, 'tasks': 1, 'like': 1, 'modeling': 1, 'text': 1, 'generation': 1, 'despite': 1, 'these': 2, 'advancements': 1, 'still': 1, 'open': 1, 'problems': 1, 'understanding': 1, 'context': 1, 'handling': 1, 'low-resource': 1, 'languages': 1, 'researchers': 1, 'continually': 1, 'developing': 1, 'new': 1, 'methods': 1, 'models': 1, 'address': 1, 'improve': 1, 'performance': 1, 'systems': 1}\n"
     ]
    }
   ],
   "source": [
    "preprocessed_documents = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "print(f\"{len(preprocessed_documents)=} | {len(corpus)=}\\n\")\n",
    "print(preprocessed_documents)\n",
    "\n",
    "# Create word_freq = {}\n",
    "for tokens in preprocessed_documents:\n",
    "    for word in tokens:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "print(f\"{len(word_freq)=}\\n\")\n",
    "print(word_freq) # a dictionary to store the frequency of each word in the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that our vocabulary has grown! We now have 140 different words - of *tokens* as we call them in NLP - in our corpus. \n",
    "\n",
    "\n",
    "##### Converting Text to Numerical Vectors\n",
    "\n",
    "Once we have preprocessed our corpus and built a vocabulary of unique words (or “tokens”), the next logical step in most text processing workflows is to transform each document into a numerical vector. This transformation is a crucial step because many (most) statistical and machine learning algorithms require numeric input. Converting text to vectors allows us to use common methods such as clustering, classification, and regression on textual data.\n",
    "\n",
    "We will construct a so-called “**document-term matrix**” structure, where rows represent documents and columns represent distinct words (from the vocabulary), is easy to manipulate and analyze. This enables us to apply a wide range of analytic techniques—from computing simple frequencies to more advanced weighting methods like TF-IDF - as we will see in the next sections.\n",
    "\n",
    "In the code below:\n",
    "- We create an index mapping for each word to its corresponding column in the “document-term matrix.”\n",
    "- We iterate over each document to produce vectors containing the count of each word in that document.\n",
    "\n",
    "This process lays the foundation for many text analysis tasks. Once documents are transformed into numerical vectors, you can apply standard methods like *K*-means clustering, logistic regression, or even neural networks to glean insights and build predictive models based on text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary Size: 94 | Number of Documents: 8\n",
      "\n",
      "Document 0 vector:\n",
      "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "\n",
      "Document 0 text:\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'of', 'study', 'which', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language']\n",
      "\n",
      "##################################################\n",
      "\n",
      "Document 1 vector:\n",
      "[1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "\n",
      "Document 1 text:\n",
      "['the', 'goal', 'of', 'nlp', 'is', 'to', 'enable', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'useful']\n",
      "\n",
      "##################################################\n",
      "\n",
      "Document 2 vector:\n",
      "[0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
      "\n",
      "Document 2 text:\n",
      "['there', 'are', 'many', 'challenges', 'in', 'nlp', 'such', 'as', 'dealing', 'with', 'the', 'ambiguity', 'and', 'variability', 'of', 'natural', 'language']\n",
      "\n",
      "##################################################\n",
      "\n",
      "Document 3 vector:\n",
      "[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Document 3 text:\n",
      "['techniques', 'in', 'nlp', 'include', 'tokenization', 'stemming', 'lemmatization', 'and', 'part-of-speech', 'tagging', 'among', 'others']\n",
      "\n",
      "##################################################\n",
      "\n",
      "Document 4 vector:\n",
      "[0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "\n",
      "Document 4 text:\n",
      "['applications', 'of', 'nlp', 'are', 'vast', 'and', 'include', 'machine', 'translation', 'sentiment', 'analysis', 'and', 'speech', 'recognition']\n",
      "\n",
      "##################################################\n",
      "\n",
      "Document 5 vector:\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "Document 5 text:\n",
      "['in', 'recent', 'years', 'deep', 'learning', 'has', 'revolutionized', 'nlp', 'leading', 'to', 'significant', 'improvements', 'in', 'tasks', 'like', 'language', 'modeling', 'and', 'text', 'generation']\n",
      "\n",
      "##################################################\n",
      "\n",
      "Document 6 vector:\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Document 6 text:\n",
      "['despite', 'these', 'advancements', 'there', 'are', 'still', 'many', 'open', 'problems', 'in', 'nlp', 'such', 'as', 'understanding', 'context', 'and', 'handling', 'low-resource', 'languages']\n",
      "\n",
      "##################################################\n",
      "\n",
      "Document 7 vector:\n",
      "[0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Document 7 text:\n",
      "['researchers', 'in', 'nlp', 'are', 'continually', 'developing', 'new', 'methods', 'and', 'models', 'to', 'address', 'these', 'challenges', 'and', 'improve', 'the', 'performance', 'of', 'nlp', 'systems']\n",
      "\n",
      "##################################################\n",
      "\n",
      "Number of documents: 8\n",
      "Vocabulary size: 94\n"
     ]
    }
   ],
   "source": [
    "def build_document_term_matrix(preprocessed_documents):\n",
    "    \"\"\"\n",
    "    Builds a Bag-of-Words document-term matrix.\n",
    "    Returns:\n",
    "    - document_term_matrix (list of lists): BoW representation of documents\n",
    "    - vocab (list): Sorted vocabulary of unique words\n",
    "    - word_to_index (dict): Mapping of word → index in vocabulary\n",
    "    \"\"\"\n",
    "    # 1. Extract unique vocabulary from the dataset and sort it\n",
    "    vocab = sorted(set(word for doc in preprocessed_documents for word in doc))\n",
    "\n",
    "    # 2. Create a word-to-index mapping\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    # 3. Convert each document into a vector\n",
    "    document_term_matrix = []\n",
    "    \n",
    "    for tokens in preprocessed_documents:\n",
    "        doc_vector = [0] * len(vocab)  # Initialize a vector of zeros\n",
    "        for token in tokens:\n",
    "            if token in word_to_index:\n",
    "                doc_vector[word_to_index[token]] += 1  # Increment word count\n",
    "        document_term_matrix.append(doc_vector)\n",
    "\n",
    "    print(f\"\\nVocabulary Size: {len(vocab)} | Number of Documents: {len(document_term_matrix)}\\n\")\n",
    "    # Let's print the result in a readable way\n",
    "    for i, vector in enumerate(document_term_matrix):\n",
    "        print(f\"Document {i} vector:\\n{vector}\\n\")\n",
    "        print(f\"Document {i} text:\\n{preprocessed_documents[i]}\\n\\n{'#'*50}\\n\")\n",
    "\n",
    "    # Optionally, we can inspect the shape of our document-term matrix\n",
    "    print(f\"Number of documents: {len(document_term_matrix)}\")\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    return document_term_matrix, vocab, word_to_index\n",
    "\n",
    "document_term_matrix, vocab, word_to_index = build_document_term_matrix(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also convert this to a Pandas DataFrame for better visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>address</th>\n",
       "      <th>advancements</th>\n",
       "      <th>ambiguity</th>\n",
       "      <th>among</th>\n",
       "      <th>analysis</th>\n",
       "      <th>and</th>\n",
       "      <th>applications</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>...</th>\n",
       "      <th>understand</th>\n",
       "      <th>understanding</th>\n",
       "      <th>useful</th>\n",
       "      <th>using</th>\n",
       "      <th>variability</th>\n",
       "      <th>vast</th>\n",
       "      <th>way</th>\n",
       "      <th>which</th>\n",
       "      <th>with</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Natural Language Processing (NLP) is a fascinating field of study, which involves the interaction between computers and humans using natural language.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>There are many challenges in NLP, such as dealing with the ambiguity and variability of natural language.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Techniques in NLP include tokenization, stemming, lemmatization, and part-of-speech tagging, among others.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Applications of NLP are vast and include machine translation, sentiment analysis, and speech recognition.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In recent years, deep learning has revolutionized NLP, leading to significant improvements in tasks like language modeling and text generation.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Despite these advancements, there are still many open problems in NLP, such as understanding context and handling low-resource languages.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Researchers in NLP are continually developing new methods and models to address these challenges and improve the performance of NLP systems.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    a  address  advancements  \\\n",
       "Natural Language Processing (NLP) is a fascinat...  1        0             0   \n",
       "The goal of NLP is to enable computers to under...  1        0             0   \n",
       "There are many challenges in NLP, such as deali...  0        0             0   \n",
       "Techniques in NLP include tokenization, stemmin...  0        0             0   \n",
       "Applications of NLP are vast and include machin...  0        0             0   \n",
       "In recent years, deep learning has revolutioniz...  0        0             0   \n",
       "Despite these advancements, there are still man...  0        0             1   \n",
       "Researchers in NLP are continually developing n...  0        1             0   \n",
       "\n",
       "                                                    ambiguity  among  \\\n",
       "Natural Language Processing (NLP) is a fascinat...          0      0   \n",
       "The goal of NLP is to enable computers to under...          0      0   \n",
       "There are many challenges in NLP, such as deali...          1      0   \n",
       "Techniques in NLP include tokenization, stemmin...          0      1   \n",
       "Applications of NLP are vast and include machin...          0      0   \n",
       "In recent years, deep learning has revolutioniz...          0      0   \n",
       "Despite these advancements, there are still man...          0      0   \n",
       "Researchers in NLP are continually developing n...          0      0   \n",
       "\n",
       "                                                    analysis  and  \\\n",
       "Natural Language Processing (NLP) is a fascinat...         0    1   \n",
       "The goal of NLP is to enable computers to under...         0    2   \n",
       "There are many challenges in NLP, such as deali...         0    1   \n",
       "Techniques in NLP include tokenization, stemmin...         0    1   \n",
       "Applications of NLP are vast and include machin...         1    2   \n",
       "In recent years, deep learning has revolutioniz...         0    1   \n",
       "Despite these advancements, there are still man...         0    1   \n",
       "Researchers in NLP are continually developing n...         0    2   \n",
       "\n",
       "                                                    applications  are  as  \\\n",
       "Natural Language Processing (NLP) is a fascinat...             0    0   0   \n",
       "The goal of NLP is to enable computers to under...             0    0   0   \n",
       "There are many challenges in NLP, such as deali...             0    1   1   \n",
       "Techniques in NLP include tokenization, stemmin...             0    0   0   \n",
       "Applications of NLP are vast and include machin...             1    1   0   \n",
       "In recent years, deep learning has revolutioniz...             0    0   0   \n",
       "Despite these advancements, there are still man...             0    1   1   \n",
       "Researchers in NLP are continually developing n...             0    1   0   \n",
       "\n",
       "                                                    ...  understand  \\\n",
       "Natural Language Processing (NLP) is a fascinat...  ...           0   \n",
       "The goal of NLP is to enable computers to under...  ...           1   \n",
       "There are many challenges in NLP, such as deali...  ...           0   \n",
       "Techniques in NLP include tokenization, stemmin...  ...           0   \n",
       "Applications of NLP are vast and include machin...  ...           0   \n",
       "In recent years, deep learning has revolutioniz...  ...           0   \n",
       "Despite these advancements, there are still man...  ...           0   \n",
       "Researchers in NLP are continually developing n...  ...           0   \n",
       "\n",
       "                                                    understanding  useful  \\\n",
       "Natural Language Processing (NLP) is a fascinat...              0       0   \n",
       "The goal of NLP is to enable computers to under...              0       1   \n",
       "There are many challenges in NLP, such as deali...              0       0   \n",
       "Techniques in NLP include tokenization, stemmin...              0       0   \n",
       "Applications of NLP are vast and include machin...              0       0   \n",
       "In recent years, deep learning has revolutioniz...              0       0   \n",
       "Despite these advancements, there are still man...              1       0   \n",
       "Researchers in NLP are continually developing n...              0       0   \n",
       "\n",
       "                                                    using  variability  vast  \\\n",
       "Natural Language Processing (NLP) is a fascinat...      1            0     0   \n",
       "The goal of NLP is to enable computers to under...      0            0     0   \n",
       "There are many challenges in NLP, such as deali...      0            1     0   \n",
       "Techniques in NLP include tokenization, stemmin...      0            0     0   \n",
       "Applications of NLP are vast and include machin...      0            0     1   \n",
       "In recent years, deep learning has revolutioniz...      0            0     0   \n",
       "Despite these advancements, there are still man...      0            0     0   \n",
       "Researchers in NLP are continually developing n...      0            0     0   \n",
       "\n",
       "                                                    way  which  with  years  \n",
       "Natural Language Processing (NLP) is a fascinat...    0      1     0      0  \n",
       "The goal of NLP is to enable computers to under...    1      0     0      0  \n",
       "There are many challenges in NLP, such as deali...    0      0     1      0  \n",
       "Techniques in NLP include tokenization, stemmin...    0      0     0      0  \n",
       "Applications of NLP are vast and include machin...    0      0     0      0  \n",
       "In recent years, deep learning has revolutioniz...    0      0     0      1  \n",
       "Despite these advancements, there are still man...    0      0     0      0  \n",
       "Researchers in NLP are continually developing n...    0      0     0      0  \n",
       "\n",
       "[8 rows x 94 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(document_term_matrix, columns=vocab).set_index(pd.Index(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for each document, we have a vector that represents the frequency of each word in the document. \n",
    "\n",
    "\n",
    "In other words, we have created basic numerical features from the text data. This could be our X_train for a machine learning model, for example.\n",
    "\n",
    "Let's try to visualize the *n* most frequent words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_top_n_tokens(n, document_term_matrix):\n",
    "\n",
    "    # Sum the occurrences of each word across all documents\n",
    "    word_counts = np.sum(document_term_matrix, axis=0)\n",
    "\n",
    "    # Get the indices of the top n most common words\n",
    "    top_n_indices = np.argsort(word_counts)[-n:]\n",
    "\n",
    "    # Get the corresponding words and their counts\n",
    "    top_n_words = [vocab[i] for i in top_n_indices]\n",
    "    top_n_counts = word_counts[top_n_indices]\n",
    "\n",
    "    # Create the bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_n_words, top_n_counts, color='skyblue')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.title(f'Top {n} Most Common Words in Document Term Matrix')\n",
    "    plt.show()\n",
    "\n",
    "visualize_top_n_tokens(10, document_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What pattern do you see? \n",
    "\n",
    "For one, we see that a lot of the top words are common English words like \"the,\" \"and,\" \"a,\" and so on. These are called **stop words** and are usually removed from the text before building a bag-of-words model. We will see how to do this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words\n",
    "\n",
    "Stop words are common words that are usually removed from the text before building a bag-of-words model. These words are usually common English words like \"the,\" \"and,\" \"a,\" and so on. These words are removed because they do not provide any useful information for the model, seeing as they are common across all texts. If, for example, we were to build a model to predict the genre of a movie based on its plot, the word \"movie\" would not be useful because it is common across all genres. Neither would the world \"the\" or \"and.\" Therefore, it is common practice to remove these words from the text before building a bag-of-words model. \n",
    "\n",
    "Let's revisit our preprocessing step and remove the stop words from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common English stop words\n",
    "STOP_WORDS = ['a', 'an', 'the', 'is', 'are', 'of', 'in', 'and', 'to', 'for', 'with', 'on', 'by', 'as', 'at', 'from', 'that', 'which', 'who', 'whom', 'whose', 'where', 'when', 'why', 'how', 'what', 's', 't', 'll', 've', 're']  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we will often use a pre-built list of stop words, such as the one provided by the NLTK library or the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hereafter', 'give', 'over', 'she', 'sometimes', 'among', 'itself', 'am', 'inc', 'herein', 'because', 'sincere', 'those', 'beyond', 'co', 'across', 'three', 'behind', 'describe', 'amount', 'herself', 'via', 'top', 'same', 'they', 'almost', 'been', 'themselves', 'so', 'though', 'cant', 'put', 'as', 'out', 'somehow', 'last', 'should', 'be', 'around', 'eg', 'must', 'of', 'thru', 'within', 're', 'most', 'always', 'during', 'your', 'the', 'thereby', 'anyhow', 'please', 'very', 'else', 'without', 'beside', 'or', 'has', 'again', 'besides', 'nobody', 'then', 'all', 'per', 'cannot', 'nor', 'everywhere', 'under', 'until', 'four', 'ten', 'move', 'yet', 'i', 'latter', 'detail', 'beforehand', 'below', 'our', 'them', 'take', 'his', 'twenty', 'now', 'otherwise', 'himself', 'although', 'back', 'once', 'anywhere', 'hundred', 'becoming', 'only', 'mine', 'anyone', 'sixty', 'thin', 'front', 'cry', 'least', 'what', 'whose', 'nevertheless', 'either', 'when', 'its', 'fill', 'ever', 'anyway', 'being', 'never', 'while', 'rather', 'us', 'seemed', 'thence', 'further', 'are', 'five', 'had', 'whereby', 'not', 'have', 'through', 'their', 'seem', 'noone', 'this', 'in', 'might', 'who', 'own', 'go', 'moreover', 'see', 'toward', 'became', 'something', 'thick', 'get', 'from', 'six', 'some', 'above', 'everyone', 'yourself', 'hereby', 'hers', 'an', 'however', 'everything', 'afterwards', 'mill', 'name', 'eight', 'ltd', 'less', 'other', 'any', 'nowhere', 'fifty', 'bill', 'but', 'hereupon', 'meanwhile', 'formerly', 'these', 'each', 'made', 'whereas', 'con', 'much', 'there', 'etc', 'someone', 'by', 'part', 'before', 'would', 'myself', 'still', 'up', 'throughout', 'former', 'bottom', 'on', 'it', 'me', 'found', 'is', 'serious', 'hence', 'except', 'few', 'others', 'why', 'since', 'whence', 'several', 'show', 'a', 'if', 'side', 'third', 'done', 'even', 'every', 'you', 'yours', 'after', 'nine', 'along', 'for', 'mostly', 'due', 'which', 'yourselves', 'whereupon', 'namely', 'anything', 'whole', 'hasnt', 'ourselves', 'call', 'enough', 'could', 'where', 'system', 'ie', 'towards', 'full', 'onto', 'de', 'about', 'amoungst', 'were', 'couldnt', 'interest', 'too', 'such', 'already', 'between', 'thus', 'fifteen', 'her', 'latterly', 'whatever', 'keep', 'whom', 'nothing', 'thereupon', 'can', 'with', 'do', 'may', 'one', 'none', 'he', 'un', 'two', 'ours', 'becomes', 'sometime', 'therefore', 'also', 'well', 'perhaps', 'him', 'and', 'many', 'off', 'against', 'than', 'upon', 'empty', 'seeming', 'whether', 'forty', 'eleven', 'neither', 'thereafter', 'will', 'become', 'down', 'find', 'wherever', 'more', 'whereafter', 'how', 'into', 'whither', 'here', 'whoever', 'whenever', 'fire', 'was', 'elsewhere', 'another', 'amongst', 'next', 'at', 'to', 'alone', 'twelve', 'often', 'somewhere', 'wherein', 'first', 'together', 'we', 'both', 'that', 'no', 'my', 'indeed', 'therein', 'seems']\n",
      "Number of stop words: 318\n"
     ]
    }
   ],
   "source": [
    "# load stopwords from scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as STOP_WORDS  # noqa\n",
    "\n",
    "print(list(STOP_WORDS))\n",
    "print(f\"Number of stop words: {len(STOP_WORDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(preprocessed_documents_no_stop)=8 | len(corpus)=8\n",
      "\n",
      "Tokens before removing stopwords: 140 | Tokens after removing stopwords: 72\n",
      "\n",
      "{'natural': 3, 'language': 5, 'processing': 1, 'nlp': 9, 'fascinating': 1, 'field': 1, 'study': 1, 'involves': 1, 'interaction': 1, 'computers': 2, 'humans': 1, 'using': 1, 'goal': 1, 'enable': 1, 'understand': 1, 'interpret': 1, 'generate': 1, 'human': 1, 'way': 1, 'meaningful': 1, 'useful': 1, 'challenges': 2, 'dealing': 1, 'ambiguity': 1, 'variability': 1, 'techniques': 1, 'include': 2, 'tokenization': 1, 'stemming': 1, 'lemmatization': 1, 'part-of-speech': 1, 'tagging': 1, 'applications': 1, 'vast': 1, 'machine': 1, 'translation': 1, 'sentiment': 1, 'analysis': 1, 'speech': 1, 'recognition': 1, 'recent': 1, 'years': 1, 'deep': 1, 'learning': 1, 'revolutionized': 1, 'leading': 1, 'significant': 1, 'improvements': 1, 'tasks': 1, 'like': 1, 'modeling': 1, 'text': 1, 'generation': 1, 'despite': 1, 'advancements': 1, 'open': 1, 'problems': 1, 'understanding': 1, 'context': 1, 'handling': 1, 'low-resource': 1, 'languages': 1, 'researchers': 1, 'continually': 1, 'developing': 1, 'new': 1, 'methods': 1, 'models': 1, 'address': 1, 'improve': 1, 'performance': 1, 'systems': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess the text\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# Remove stop words\n",
    "preprocessed_documents_no_stop = [\n",
    "    [token for token in document if token not in STOP_WORDS]\n",
    "    for document in preprocessed_documents\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"{len(preprocessed_documents_no_stop)=} | {len(corpus)=}\\n\")\n",
    "\n",
    "no_stopword_freq = {}\n",
    "for tokens in preprocessed_documents_no_stop:\n",
    "    for word in tokens:\n",
    "        if word in no_stopword_freq:\n",
    "            no_stopword_freq[word] += 1\n",
    "        else:\n",
    "            no_stopword_freq[word] = 1\n",
    "\n",
    "print(f\"Tokens before removing stopwords: {len(word_freq)} | Tokens after removing stopwords: {len(no_stopword_freq)}\\n\")\n",
    "print(no_stopword_freq) # a dictionary to store the frequency of each word in the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_top_n_words(freq_dict, n=10):\n",
    "    \"\"\"\n",
    "    Plots a horizontal bar chart of the top n most frequent words\n",
    "    from a given frequency dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Sort the dictionary by frequency in descending order.\n",
    "    sorted_freq = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 2. Slice the top n words and their frequencies.\n",
    "    top_n = sorted_freq[:n]\n",
    "    \n",
    "    # 3. Separate the words and their frequencies for plotting.\n",
    "    words = [item[0] for item in top_n]\n",
    "    counts = [item[1] for item in top_n]\n",
    "    \n",
    "    # 4. Create a bar chart.\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(words, counts, color='skyblue')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.title(f'Top {n} Words by Frequency (Excluding Stopwords)')\n",
    "    \n",
    "    # Invert y-axis so the highest frequency word appears at the top\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # 5. Display the plot.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_top_n_words(no_stopword_freq, n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we removed a lot of the common English words, but perhaps not all. This is because the list of stop words is not exhaustive. You can find many different list of stop words for different languages. You might also consider that we are removing words that could be useful for our model. For example, the word \"not\" could be useful for sentiment analysis. Therefore, it is important to consider the context of your model when removing stop words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization\n",
    "\n",
    "Another common preprocessing step is **stemming** and **lemmatization**. These are techniques used to reduce words to their root form. For example, the words \"running,\" \"runs,\" and \"ran\" all have the same root word \"run.\" Stemming and lemmatization are used to reduce words to their root form so that the model can understand that these words are the same. \n",
    "\n",
    "* **Stemming**: Stemming is the process of reducing words to their root form by removing suffixes. For example, the words \"running,\" \"runs,\" and \"ran\" would all be reduced to \"run.\" There are many different stemming algorithms, but one of the most common is the Porter Stemmer.\n",
    "\n",
    "* **Lemmatization**: Lemmatization is the process of reducing words to their root form by removing suffixes and prefixes. Lemmatization is more sophisticated than stemming because it uses a vocabulary and morphological analysis to reduce words to their root form. For example, the words \"am,\" \"are,\" and \"is\" would all be reduced to \"be.\"\n",
    "\n",
    "To do so, let's update our `preprocess_text` function to include stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Running, run, and runs are different forms of the same root word!\n",
      "Output: ['run', 'run', 'run', 'differ', 'form', 'root', 'word']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nicolai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary NLTK resources (if not already installed)\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing punctuation\n",
    "    3. Tokenizing (splitting into words)\n",
    "    4. Applying stemming to reduce words to their root forms\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Porter Stemmer\n",
    "    stemmer = PorterStemmer()  # <--- THIS IS NEW: Initialize the stemmer\n",
    "\n",
    "    # 1. Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove punctuation by keeping only alphanumeric characters and spaces\n",
    "    punctuation_chars = \".,!?;:'\\\"()\"\n",
    "    text = ''.join(char for char in text if char not in punctuation_chars)\n",
    "\n",
    "    # 3. Tokenize by splitting on whitespace\n",
    "    tokens = text.split()\n",
    "\n",
    "    # 4. Apply stemming to each token\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # <--- THIS IS NEW: Apply stemming\n",
    "\n",
    "    # 5. remove stopwords\n",
    "    stemmed_tokens = [word for word in stemmed_tokens if word not in STOP_WORDS]\n",
    "\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Example usage\n",
    "document = \"Running, run, and runs are different forms of the same root word!\"\n",
    "tokens = preprocess_text(document)\n",
    "print(f\"Input: {document}\")\n",
    "print(f\"Output: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidating all tokens with the same root word can help reduce the size of the vocabulary and improve the model's performance!\n",
    "\n",
    "Let's built our document-term matrix again, but this time with stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'nlp', 'fascin', 'field', 'studi', 'involv', 'interact', 'comput', 'human', 'use', 'natur', 'languag']\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = [preprocess_text(doc) for doc in corpus]\n",
    "print(tokenized_docs[0])  # Display the first two tokenized documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary Size: 66 | Number of Documents: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_document_term_matrix(preprocessed_documents):\n",
    "    \"\"\"\n",
    "    Builds a Bag-of-Words document-term matrix.\n",
    "    Returns:\n",
    "    - document_term_matrix (list of lists): BoW representation of documents\n",
    "    - vocab (list): Sorted vocabulary of unique words\n",
    "    - word_to_index (dict): Mapping of word → index in vocabulary\n",
    "    \"\"\"\n",
    "    # 1. Extract unique vocabulary from the dataset and sort it\n",
    "    vocab = sorted(set(word for doc in preprocessed_documents for word in doc))\n",
    "\n",
    "    # 2. Create a word-to-index mapping\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    # 3. Convert each document into a vector\n",
    "    document_term_matrix = []\n",
    "    \n",
    "    for tokens in preprocessed_documents:\n",
    "        doc_vector = [0] * len(vocab)  # Initialize a vector of zeros\n",
    "        for token in tokens:\n",
    "            if token in word_to_index:\n",
    "                doc_vector[word_to_index[token]] += 1  # Increment word count\n",
    "        document_term_matrix.append(doc_vector)\n",
    "\n",
    "    print(f\"\\nVocabulary Size: {len(vocab)} | Number of Documents: {len(document_term_matrix)}\\n\")\n",
    "    return document_term_matrix, vocab, word_to_index\n",
    "\n",
    "document_term_matrix, vocab, word_to_index = build_document_term_matrix(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>advanc</th>\n",
       "      <th>ambigu</th>\n",
       "      <th>analysi</th>\n",
       "      <th>applic</th>\n",
       "      <th>challeng</th>\n",
       "      <th>comput</th>\n",
       "      <th>context</th>\n",
       "      <th>continu</th>\n",
       "      <th>deal</th>\n",
       "      <th>...</th>\n",
       "      <th>techniqu</th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "      <th>translat</th>\n",
       "      <th>understand</th>\n",
       "      <th>use</th>\n",
       "      <th>variabl</th>\n",
       "      <th>vast</th>\n",
       "      <th>way</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Natural Language Processing (NLP) is a fascinating field of study, which involves the interaction between computers and humans using natural language.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>There are many challenges in NLP, such as dealing with the ambiguity and variability of natural language.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Techniques in NLP include tokenization, stemming, lemmatization, and part-of-speech tagging, among others.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Applications of NLP are vast and include machine translation, sentiment analysis, and speech recognition.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In recent years, deep learning has revolutionized NLP, leading to significant improvements in tasks like language modeling and text generation.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Despite these advancements, there are still many open problems in NLP, such as understanding context and handling low-resource languages.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Researchers in NLP are continually developing new methods and models to address these challenges and improve the performance of NLP systems.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    address  advanc  ambigu  \\\n",
       "Natural Language Processing (NLP) is a fascinat...        0       0       0   \n",
       "The goal of NLP is to enable computers to under...        0       0       0   \n",
       "There are many challenges in NLP, such as deali...        0       0       1   \n",
       "Techniques in NLP include tokenization, stemmin...        0       0       0   \n",
       "Applications of NLP are vast and include machin...        0       0       0   \n",
       "In recent years, deep learning has revolutioniz...        0       0       0   \n",
       "Despite these advancements, there are still man...        0       1       0   \n",
       "Researchers in NLP are continually developing n...        1       0       0   \n",
       "\n",
       "                                                    analysi  applic  challeng  \\\n",
       "Natural Language Processing (NLP) is a fascinat...        0       0         0   \n",
       "The goal of NLP is to enable computers to under...        0       0         0   \n",
       "There are many challenges in NLP, such as deali...        0       0         1   \n",
       "Techniques in NLP include tokenization, stemmin...        0       0         0   \n",
       "Applications of NLP are vast and include machin...        1       1         0   \n",
       "In recent years, deep learning has revolutioniz...        0       0         0   \n",
       "Despite these advancements, there are still man...        0       0         0   \n",
       "Researchers in NLP are continually developing n...        0       0         1   \n",
       "\n",
       "                                                    comput  context  continu  \\\n",
       "Natural Language Processing (NLP) is a fascinat...       1        0        0   \n",
       "The goal of NLP is to enable computers to under...       1        0        0   \n",
       "There are many challenges in NLP, such as deali...       0        0        0   \n",
       "Techniques in NLP include tokenization, stemmin...       0        0        0   \n",
       "Applications of NLP are vast and include machin...       0        0        0   \n",
       "In recent years, deep learning has revolutioniz...       0        0        0   \n",
       "Despite these advancements, there are still man...       0        1        0   \n",
       "Researchers in NLP are continually developing n...       0        0        1   \n",
       "\n",
       "                                                    deal  ...  techniqu  text  \\\n",
       "Natural Language Processing (NLP) is a fascinat...     0  ...         0     0   \n",
       "The goal of NLP is to enable computers to under...     0  ...         0     0   \n",
       "There are many challenges in NLP, such as deali...     1  ...         0     0   \n",
       "Techniques in NLP include tokenization, stemmin...     0  ...         1     0   \n",
       "Applications of NLP are vast and include machin...     0  ...         0     0   \n",
       "In recent years, deep learning has revolutioniz...     0  ...         0     1   \n",
       "Despite these advancements, there are still man...     0  ...         0     0   \n",
       "Researchers in NLP are continually developing n...     0  ...         0     0   \n",
       "\n",
       "                                                    token  translat  \\\n",
       "Natural Language Processing (NLP) is a fascinat...      0         0   \n",
       "The goal of NLP is to enable computers to under...      0         0   \n",
       "There are many challenges in NLP, such as deali...      0         0   \n",
       "Techniques in NLP include tokenization, stemmin...      1         0   \n",
       "Applications of NLP are vast and include machin...      0         1   \n",
       "In recent years, deep learning has revolutioniz...      0         0   \n",
       "Despite these advancements, there are still man...      0         0   \n",
       "Researchers in NLP are continually developing n...      0         0   \n",
       "\n",
       "                                                    understand  use  variabl  \\\n",
       "Natural Language Processing (NLP) is a fascinat...           0    1        0   \n",
       "The goal of NLP is to enable computers to under...           1    1        0   \n",
       "There are many challenges in NLP, such as deali...           0    0        1   \n",
       "Techniques in NLP include tokenization, stemmin...           0    0        0   \n",
       "Applications of NLP are vast and include machin...           0    0        0   \n",
       "In recent years, deep learning has revolutioniz...           0    0        0   \n",
       "Despite these advancements, there are still man...           1    0        0   \n",
       "Researchers in NLP are continually developing n...           0    0        0   \n",
       "\n",
       "                                                    vast  way  year  \n",
       "Natural Language Processing (NLP) is a fascinat...     0    0     0  \n",
       "The goal of NLP is to enable computers to under...     0    1     0  \n",
       "There are many challenges in NLP, such as deali...     0    0     0  \n",
       "Techniques in NLP include tokenization, stemmin...     0    0     0  \n",
       "Applications of NLP are vast and include machin...     1    0     0  \n",
       "In recent years, deep learning has revolutioniz...     0    0     1  \n",
       "Despite these advancements, there are still man...     0    0     0  \n",
       "Researchers in NLP are continually developing n...     0    0     0  \n",
       "\n",
       "[8 rows x 66 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(document_term_matrix, columns=vocab).set_index(pd.Index(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now have a pretty decent bag-of-words model! We have preprocessed the text, removed stop words, and stemmed the words. We have also built a document-term matrix that represents the frequency of each word in each document. But how do we use it as an encoder? This will require us to\n",
    "\n",
    "1. **Fit**: Build the vocabulary and the document-term matrix from the training data.\n",
    "2. **Transform**: Use the fitted vocabulary to transform the test data into a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary Size: 66 | Number of Documents: 8\n",
      "\n",
      "\n",
      "Encoded New Documents:\n",
      "New Document 0 vector:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "New Document 1 vector:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "New Document 2 vector:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transform_new_document(new_documents, vocab, word_to_index):\n",
    "    \"\"\"\n",
    "    Transforms new, unseen documents into the existing BoW format.\n",
    "    Ensures the new documents match the training vocabulary structure.\n",
    "    \"\"\"\n",
    "    new_doc_matrix = []\n",
    "\n",
    "    for tokens in new_documents:\n",
    "        doc_vector = [0] * len(vocab)  # Initialize vector with zeros\n",
    "        for token in tokens:\n",
    "            if token in word_to_index:\n",
    "                doc_vector[word_to_index[token]] += 1  # Increment word count\n",
    "        new_doc_matrix.append(doc_vector)\n",
    "\n",
    "    return new_doc_matrix\n",
    "\n",
    "\n",
    "document_term_matrix, vocab, word_to_index = build_document_term_matrix(tokenized_docs)\n",
    "\n",
    "# New Data (Unseen)\n",
    "new_docs = [\n",
    "    [\"nlp\", \"is\", \"fun\"],\n",
    "    [\"i\", \"love\", \"space\"],\n",
    "    [\"sports\", \"are\", \"amazing\"]\n",
    "]\n",
    "\n",
    "# Transform the new documents using the existing vocabulary\n",
    "new_doc_vectors = transform_new_document(new_docs, vocab, word_to_index)\n",
    "\n",
    "print(\"\\nEncoded New Documents:\")\n",
    "for i, vector in enumerate(new_doc_vectors):\n",
    "    print(f\"New Document {i} vector:\\n{vector}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automating Bag-of-Words Processing with CountVectorizer\n",
    "\n",
    "So far, we have manually implemented the **Bag-of-Words (BoW) model** by performing the following preprocessing steps:\n",
    "\n",
    "1. **Lowercasing** – To ensure that words like \"The\" and \"the\" are treated as the same token.  \n",
    "2. **Removing Punctuation** – To prevent punctuation marks from interfering with tokenization.  \n",
    "3. **Removing Stopwords** – To eliminate common words that do not carry much information.\n",
    "4. **Tokenization** – To split text into individual words (tokens).  \n",
    "5. **Stemming** – To reduce words to their root forms (e.g., \"running\" → \"run\").  \n",
    "\n",
    "While implementing these steps manually is useful for understanding text preprocessing, in real-world applications, we often use libraries like **Scikit-learn’s `CountVectorizer`**, which automates much of this process. `CountVectorizer`:\n",
    "\n",
    "- Converts text into a numerical matrix where each column represents a unique word.\n",
    "- Tokenizes and processes text efficiently.\n",
    "- Allows filtering of stopwords and setting custom preprocessing options.\n",
    "\n",
    "### **But what about Stemming?**\n",
    "One limitation of `CountVectorizer` is that **it does not include stemming by default**. However, we can integrate stemming by **creating a custom tokenizer** that applies stemming before passing tokens to `CountVectorizer`.\n",
    "\n",
    "### **Solution: Custom Preprocessing for CountVectorizer**\n",
    "Instead of using the default tokenization in `CountVectorizer`, we will:\n",
    "1. Write a **custom tokenizer** that applies stemming.\n",
    "2. Pass this tokenizer into `CountVectorizer` via the `tokenizer` argument.\n",
    "\n",
    "This approach allows us to combine the **power of `CountVectorizer`** with the **benefits of stemming**, ensuring that words like *running*, *runs*, and *runner* are all treated as the same token.\n",
    "\n",
    "## Newsgroups Dataset\n",
    "Before we go on, let's switch to a more exciting dataset. We will use the **20 newsgroups** dataset, which is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. This dataset is often used for text classification and clustering tasks. For this tutorial, we will use a subset of the dataset containing only 2 categories: `sci.space` and `rec.sport.baseball`.\n",
    "\n",
    "We will apply the concepts we have learned so far to this dataset. Let's start by loading the dataset and exploring it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 1190\n",
      "Testing data size: 791\n",
      "Example document:\n",
      "I've been saying this for quite some time, but being absent from the\n",
      "net for a while I figured I'd stick my neck out a bit...\n",
      "\n",
      "The Royals will set the record for fewest runs scored by an AL\n",
      "team since the inception of the DH rule.  (p.s. any ideas what this is?)\n",
      "\n",
      "They will fall easily short of 600 runs, that's for damn sure.  I can't\n",
      "believe these media fools picking them to win the division (like our\n",
      "Tom Gage of the Detroit News claiming Herk Robinson is some kind of\n",
      "genius for the trades/aquisitions he's made)\n",
      "\n",
      "c-ya\n",
      "\n",
      "Sean\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the 20 newsgroups dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Define the categories\n",
    "categories = ['rec.sport.baseball', 'sci.space']\n",
    "\n",
    "# Load the training data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Load the testing data\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Display some information about the data\n",
    "print('Training data size:', len(newsgroups_train.data))\n",
    "print('Testing data size:', len(newsgroups_test.data))\n",
    "print('Example document:')\n",
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn these documents into a bag-of-words model. We will follow the same steps as before, but this time we will use the `CountVectorizer` from scikit-learn, which does (nearly) all the preprocessing steps for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiml25-ma2/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>#/usr/bin/perl</th>\n",
       "      <th>#0739</th>\n",
       "      <th>#1</th>\n",
       "      <th>#102</th>\n",
       "      <th>#14</th>\n",
       "      <th>#150</th>\n",
       "      <th>#1506</th>\n",
       "      <th>#2</th>\n",
       "      <th>#2219</th>\n",
       "      <th>...</th>\n",
       "      <th>~31</th>\n",
       "      <th>~400</th>\n",
       "      <th>~5</th>\n",
       "      <th>~50</th>\n",
       "      <th>~85</th>\n",
       "      <th>~ftp/pub/rsdwg</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>º</th>\n",
       "      <th>ñ-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I've been saying this for quite some time, but being absent from the\\nnet for a while I figured I'd stick my neck out a bit...\\n\\nThe Royals will set the record for fewest runs scored by an AL\\nteam since the inception of the DH rule.  (p.s. any ideas what this is?)\\n\\nThey will fall easily short of 600 runs, that's for damn sure.  I can't\\nbelieve these media fools picking them to win the division (like our\\nTom Gage of the Detroit News claiming Herk Robinson is some kind of\\ngenius for the trades/aquisitions he's made)\\n\\nc-ya\\n\\nSean\\n\\n</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sorry for asking a question that's not entirely based on the\\ntechnical aspects of space, but I couldn't find the\\nanswer on the FAQs !\\n\\nI'm currently in the UK, which makes seeing a Space Shuttle\\nlaunch a little difficult.....\\n\\nHowever, I have been selected to be an exchange student\\nat Louisiana State Uni. from August, and I am absolutely\\ndetermined to get to see a Space Shuttle launch sometime\\nduring the year at which I will be in America.\\n\\nI hear there's a bit of a long mailing list, so if someone\\ncan tell me how to get tickets and where to get them from, then\\nplease E-mail me !\\n\\nThanks very much for your patience....\\n\\n(And if anyone else wants to know, tell me and I'll summarize\\nfor you - just to save all those poor people who have to\\npay for their links !)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Giant's have a five man rotation of  John Burkett, Trevor Wilson,\\nBill Swift, Jeff Brantley, and Bud Black/Dave Burba.  Black has\\nbeen put on the 15 day disables and Dave Burba will take his starts.\\n\\n</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\nLot's of these small miners  are no longer miners.  THey are people living\\nrent free on Federal land,  under the claim of being a miner.  The facts are\\nmany of these people do not sustaint heir income from mining,  do not\\noften even live their full time,  and do fotentimes do a fair bit\\nof environmental damage.\\n\\nThese minign statutes were created inthe 1830's-1870's  when the west was\\nuninhabited  and were designed to bring people into the frontier.  Times change\\npeople change.  DEAL.  you don't have a constitutional right to live off\\nthe same industry forever.  Anyone who claims the have a right to their\\njob in particular,  is spouting nonsense.   THis has been a long term\\nfederal welfare program,  that has outlived it's usefulness.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The term \"stopper\" is generally used to refer to a pitcher, one\\nwho can be counted on to pitch a strong game to keep his team from going\\non a losing streak.\\n\\n\\tThe Braves have plenty of pitchers to fit this description,\\nalthough right now I'd expect Smoltz or Glavine to take the mantle.\\n\\n\\tWhat the Braves lack, however, is an offensive stopper,\\nsomebody they can look to to bring them out of their hitting slump.\\nThere's just no one there.  The Braves got rid of their best pure\\nhitter, Lonnie Smith, and only Terry Pendleton on the current roster\\nhas ever shown more than a cursory ability to hit.\\t\\n\\n\\tOh, and another thing that worries me.  Ron Gant seems to have\\nslowed down a step.  That's scary.  A slow Ron Gant doesn't have much going\\nfor him.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16415 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    #  #/usr/bin/perl  #0739  \\\n",
       "I've been saying this for quite some time, but ...  0               0      0   \n",
       "Sorry for asking a question that's not entirely...  0               0      0   \n",
       "Giant's have a five man rotation of  John Burke...  0               0      0   \n",
       "\\n\\nLot's of these small miners  are no longer ...  0               0      0   \n",
       "The term \"stopper\" is generally used to refer t...  0               0      0   \n",
       "\n",
       "                                                    #1  #102  #14  #150  \\\n",
       "I've been saying this for quite some time, but ...   0     0    0     0   \n",
       "Sorry for asking a question that's not entirely...   0     0    0     0   \n",
       "Giant's have a five man rotation of  John Burke...   0     0    0     0   \n",
       "\\n\\nLot's of these small miners  are no longer ...   0     0    0     0   \n",
       "The term \"stopper\" is generally used to refer t...   0     0    0     0   \n",
       "\n",
       "                                                    #1506  #2  #2219  ...  \\\n",
       "I've been saying this for quite some time, but ...      0   0      0  ...   \n",
       "Sorry for asking a question that's not entirely...      0   0      0  ...   \n",
       "Giant's have a five man rotation of  John Burke...      0   0      0  ...   \n",
       "\\n\\nLot's of these small miners  are no longer ...      0   0      0  ...   \n",
       "The term \"stopper\" is generally used to refer t...      0   0      0  ...   \n",
       "\n",
       "                                                    ~31  ~400  ~5  ~50  ~85  \\\n",
       "I've been saying this for quite some time, but ...    0     0   0    0    0   \n",
       "Sorry for asking a question that's not entirely...    0     0   0    0    0   \n",
       "Giant's have a five man rotation of  John Burke...    0     0   0    0    0   \n",
       "\\n\\nLot's of these small miners  are no longer ...    0     0   0    0    0   \n",
       "The term \"stopper\" is generally used to refer t...    0     0   0    0    0   \n",
       "\n",
       "                                                    ~ftp/pub/rsdwg  \\\n",
       "I've been saying this for quite some time, but ...               0   \n",
       "Sorry for asking a question that's not entirely...               0   \n",
       "Giant's have a five man rotation of  John Burke...               0   \n",
       "\\n\\nLot's of these small miners  are no longer ...               0   \n",
       "The term \"stopper\" is generally used to refer t...               0   \n",
       "\n",
       "                                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "I've been saying this for quite some time, but ...                                                  0                            \n",
       "Sorry for asking a question that's not entirely...                                                  0                            \n",
       "Giant's have a five man rotation of  John Burke...                                                  0                            \n",
       "\\n\\nLot's of these small miners  are no longer ...                                                  0                            \n",
       "The term \"stopper\" is generally used to refer t...                                                  0                            \n",
       "\n",
       "                                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "I've been saying this for quite some time, but ...                                                  0                              \n",
       "Sorry for asking a question that's not entirely...                                                  0                              \n",
       "Giant's have a five man rotation of  John Burke...                                                  0                              \n",
       "\\n\\nLot's of these small miners  are no longer ...                                                  0                              \n",
       "The term \"stopper\" is generally used to refer t...                                                  0                              \n",
       "\n",
       "                                                    º  ñ-  \n",
       "I've been saying this for quite some time, but ...  0   0  \n",
       "Sorry for asking a question that's not entirely...  0   0  \n",
       "Giant's have a five man rotation of  John Burke...  0   0  \n",
       "\\n\\nLot's of these small miners  are no longer ...  0   0  \n",
       "The term \"stopper\" is generally used to refer t...  0   0  \n",
       "\n",
       "[5 rows x 16415 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# 2️⃣ Convert text into Bag-of-Words representation\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=preprocess_text,  # <--- Use our custom preprocess_text function as the tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)  # Learn the vocabulary and transform the data into feature vectors\n",
    "X_test = vectorizer.transform(newsgroups_test.data) # Transform the test data into feature vectors (Never fit your test data!)\n",
    "\n",
    "# Target labels (1=space 0=baseball)\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "# Create a DataFrame from the vectorizer vocabulary\n",
    "vocab = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out(), index=newsgroups_train.data)\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the dataframe above that we have \n",
    "\n",
    "* Over 16.4K tokens in our vocabulary!\n",
    "* a lot of pretty strange words in our vocabulary. Tokens like \"~ftp/pub/rsdwg\" could be really useful or really noisy - depending on the context.\n",
    "\n",
    "If we wanted to remove these tokens, we could use the `min_df` and `max_df` parameters in `CountVectorizer`. These parameters allow us to set the minimum and maximum frequency of a token in the corpus. For example, if we set `min_df=0.01`, we would remove tokens that appear in less than 1% of the documents. If we set `max_df=0.9`, we would remove tokens that appear in more than 90% of the documents. Since a lot of word often only appear once, this could be a good way to remove some noise from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiml25-ma2/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$</th>\n",
       "      <th>$date</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>*</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>/</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yanke</th>\n",
       "      <th>ye</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>york</th>\n",
       "      <th>youd</th>\n",
       "      <th>young</th>\n",
       "      <th>|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I've been saying this for quite some time, but being absent from the\\nnet for a while I figured I'd stick my neck out a bit...\\n\\nThe Royals will set the record for fewest runs scored by an AL\\nteam since the inception of the DH rule.  (p.s. any ideas what this is?)\\n\\nThey will fall easily short of 600 runs, that's for damn sure.  I can't\\nbelieve these media fools picking them to win the division (like our\\nTom Gage of the Detroit News claiming Herk Robinson is some kind of\\ngenius for the trades/aquisitions he's made)\\n\\nc-ya\\n\\nSean\\n\\n</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sorry for asking a question that's not entirely based on the\\ntechnical aspects of space, but I couldn't find the\\nanswer on the FAQs !\\n\\nI'm currently in the UK, which makes seeing a Space Shuttle\\nlaunch a little difficult.....\\n\\nHowever, I have been selected to be an exchange student\\nat Louisiana State Uni. from August, and I am absolutely\\ndetermined to get to see a Space Shuttle launch sometime\\nduring the year at which I will be in America.\\n\\nI hear there's a bit of a long mailing list, so if someone\\ncan tell me how to get tickets and where to get them from, then\\nplease E-mail me !\\n\\nThanks very much for your patience....\\n\\n(And if anyone else wants to know, tell me and I'll summarize\\nfor you - just to save all those poor people who have to\\npay for their links !)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Giant's have a five man rotation of  John Burkett, Trevor Wilson,\\nBill Swift, Jeff Brantley, and Bud Black/Dave Burba.  Black has\\nbeen put on the 15 day disables and Dave Burba will take his starts.\\n\\n</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\nLot's of these small miners  are no longer miners.  THey are people living\\nrent free on Federal land,  under the claim of being a miner.  The facts are\\nmany of these people do not sustaint heir income from mining,  do not\\noften even live their full time,  and do fotentimes do a fair bit\\nof environmental damage.\\n\\nThese minign statutes were created inthe 1830's-1870's  when the west was\\nuninhabited  and were designed to bring people into the frontier.  Times change\\npeople change.  DEAL.  you don't have a constitutional right to live off\\nthe same industry forever.  Anyone who claims the have a right to their\\njob in particular,  is spouting nonsense.   THis has been a long term\\nfederal welfare program,  that has outlived it's usefulness.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The term \"stopper\" is generally used to refer to a pitcher, one\\nwho can be counted on to pitch a strong game to keep his team from going\\non a losing streak.\\n\\n\\tThe Braves have plenty of pitchers to fit this description,\\nalthough right now I'd expect Smoltz or Glavine to take the mantle.\\n\\n\\tWhat the Braves lack, however, is an offensive stopper,\\nsomebody they can look to to bring them out of their hitting slump.\\nThere's just no one there.  The Braves got rid of their best pure\\nhitter, Lonnie Smith, and only Terry Pendleton on the current roster\\nhas ever shown more than a cursory ability to hit.\\t\\n\\n\\tOh, and another thing that worries me.  Ron Gant seems to have\\nslowed down a step.  That's scary.  A slow Ron Gant doesn't have much going\\nfor him.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1044 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    $  $date  &  *  -  --  /  \\\n",
       "I've been saying this for quite some time, but ...  0      0  0  0  0   0  0   \n",
       "Sorry for asking a question that's not entirely...  0      0  0  0  1   0  0   \n",
       "Giant's have a five man rotation of  John Burke...  0      0  0  0  0   0  0   \n",
       "\\n\\nLot's of these small miners  are no longer ...  0      0  0  0  0   0  0   \n",
       "The term \"stopper\" is generally used to refer t...  0      0  0  0  0   0  0   \n",
       "\n",
       "                                                    0  1  10  ...  wrong  \\\n",
       "I've been saying this for quite some time, but ...  0  0   0  ...      0   \n",
       "Sorry for asking a question that's not entirely...  0  0   0  ...      0   \n",
       "Giant's have a five man rotation of  John Burke...  0  0   0  ...      0   \n",
       "\\n\\nLot's of these small miners  are no longer ...  0  0   0  ...      0   \n",
       "The term \"stopper\" is generally used to refer t...  0  0   0  ...      0   \n",
       "\n",
       "                                                    yanke  ye  yeah  year  \\\n",
       "I've been saying this for quite some time, but ...      0   0     0     0   \n",
       "Sorry for asking a question that's not entirely...      0   0     0     1   \n",
       "Giant's have a five man rotation of  John Burke...      0   0     0     0   \n",
       "\\n\\nLot's of these small miners  are no longer ...      0   0     0     0   \n",
       "The term \"stopper\" is generally used to refer t...      0   0     0     0   \n",
       "\n",
       "                                                    yesterday  york  youd  \\\n",
       "I've been saying this for quite some time, but ...          0     0     0   \n",
       "Sorry for asking a question that's not entirely...          0     0     0   \n",
       "Giant's have a five man rotation of  John Burke...          0     0     0   \n",
       "\\n\\nLot's of these small miners  are no longer ...          0     0     0   \n",
       "The term \"stopper\" is generally used to refer t...          0     0     0   \n",
       "\n",
       "                                                    young  |  \n",
       "I've been saying this for quite some time, but ...      0  0  \n",
       "Sorry for asking a question that's not entirely...      0  0  \n",
       "Giant's have a five man rotation of  John Burke...      0  0  \n",
       "\\n\\nLot's of these small miners  are no longer ...      0  0  \n",
       "The term \"stopper\" is generally used to refer t...      0  0  \n",
       "\n",
       "[5 rows x 1044 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2️⃣ Convert text into Bag-of-Words representation\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=preprocess_text,  # <--- Use our custom preprocess_text function as the tokenizer\n",
    "    min_df=0.012,  # Ignore terms that have a document frequency strictly lower than the given threshold\n",
    "    max_df=0.9,  # Ignore terms that have a document frequency strictly higher than the given threshold\n",
    ")\n",
    "\n",
    "\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)  # Learn the vocabulary and transform the data into feature vectors\n",
    "X_test = vectorizer.transform(newsgroups_test.data) # Transform the test data into feature vectors (Never fit your test data!)\n",
    "\n",
    "# Target labels (1=space 0=baseball)\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "# Create a DataFrame from the vectorizer vocabulary\n",
    "vocab = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out(), index=newsgroups_train.data)\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting a `min_df` of 0.012 and `max_df` to 0.9 (which means we remove tokens that appear in less than 1.2% of the documents or more than 90% of the documents), we can reduce the number of tokens in our vocabulary from ~16.4K to just over 1K!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# 3️⃣ Train a Naive Bayes classifier (good for BoW text data)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4️⃣ Predict on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 0.9178\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.90      0.94      0.92       397\n",
      "         sci.space       0.94      0.90      0.92       394\n",
      "\n",
      "          accuracy                           0.92       791\n",
      "         macro avg       0.92      0.92      0.92       791\n",
      "      weighted avg       0.92      0.92      0.92       791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 5️⃣ Evaluate Model Performance\n",
    "\n",
    "# Compute accuracy by comparing predicted labels (y_pred) with actual labels (y_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate a detailed classification report (precision, recall, f1-score)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are pretty good scores! We have successfully built a bag-of-words model for our text data and used it to classify the documents into their respective categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 6️⃣ Visualize Top Words in Each Category (Using Raw Counts)\n",
    "def plot_top_words(model, vectorizer, categories, n=10):\n",
    "    \"\"\"\n",
    "    Visualizes the top N most important words for each category based on raw word counts.\n",
    "    \n",
    "    How It Works:\n",
    "    - The model stores word counts for each class.\n",
    "    - We extract the most commonly used words in each category.\n",
    "    - We visualize the words that best differentiate \"sci.space\" from \"rec.sport.baseball\".\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained Naive Bayes model (MultinomialNB)\n",
    "    - vectorizer: CountVectorizer instance used to convert text into Bag-of-Words\n",
    "    - categories: List of class names corresponding to the model's output\n",
    "    - n: Number of top words to display per category\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocabulary: A list of all words indexed by feature position\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get the raw count of each word per class\n",
    "    word_counts = model.feature_count_  # This gives raw word occurrence counts per class\n",
    "\n",
    "    # Plot the top N words per class\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    for i, class_label in enumerate(model.classes_):\n",
    "        # Find indices of the top N words for this class based on raw counts\n",
    "        top_n_indices = np.argsort(word_counts[i])[-n:][::-1]  # Sort descending\n",
    "\n",
    "        # Get the actual words and their raw counts\n",
    "        top_words = [feature_names[j] for j in top_n_indices]\n",
    "        top_counts = [word_counts[i][j] for j in top_n_indices]\n",
    "\n",
    "        # Plot for the current class\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        plt.barh(top_words, top_counts, color='skyblue')\n",
    "        plt.xlabel(\"Word Count\")\n",
    "        plt.title(f\"Top {n} Words for {categories[i]}\")\n",
    "        plt.gca().invert_yaxis()  # Ensure highest count words appear at the top\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 🔥 Plot top words\n",
    "plot_top_words(model, vectorizer, categories, n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most frequent tokens for each category, we can see that the tokens are quite different. We still observe a few stopwords, such as \"wa\" \"ha\", \"1\", \"thi\", \"0\". These could be removed to improve the model's performance. This is left as an exercise for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advancing to TF-IDF\n",
    "\n",
    "One limitation of the bag-of-words model is that it treats all words as equally important. However, some words are more important than others in a document. For example, the word \"space\" is likely more important in a document about space than the word \"baseball.\". How might we account for this in our model?\n",
    "\n",
    "One way to address this issue is to use **Term Frequency-Inverse Document Frequency (TF-IDF)**. TF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents. It is calculated as the product of two terms:\n",
    "\n",
    "1. **Term Frequency (TF)**: The frequency of a word in a document. This is our CountVectorizer. It is calculated as the number of times a word appears in a document divided by the total number of words in the document.\n",
    "2. **Inverse Document Frequency (IDF)**: The inverse of the frequency of a word in a collection of documents. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the word.\n",
    "\n",
    "Tokens that appear frequently in a document but infrequently in other documents are considered important and receive a high TF-IDF score. Conversely, tokens that appear frequently in all documents are considered less important and receive a low TF-IDF score.\n",
    "\n",
    "Let's implement TF-IDF on our dataset. We will use the `TfidfVectorizer` from scikit-learn, which combines the CountVectorizer and TfidfTransformer into a single step. We will follow the same steps as before, but this time we will use the `TfidfVectorizer` instead of the `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiml25-ma2/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>#/usr/bin/perl</th>\n",
       "      <th>#0739</th>\n",
       "      <th>#1</th>\n",
       "      <th>#102</th>\n",
       "      <th>#14</th>\n",
       "      <th>#150</th>\n",
       "      <th>#1506</th>\n",
       "      <th>#2</th>\n",
       "      <th>#2219</th>\n",
       "      <th>...</th>\n",
       "      <th>~31</th>\n",
       "      <th>~400</th>\n",
       "      <th>~5</th>\n",
       "      <th>~50</th>\n",
       "      <th>~85</th>\n",
       "      <th>~ftp/pub/rsdwg</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</th>\n",
       "      <th>º</th>\n",
       "      <th>ñ-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I've been saying this for quite some time, but being absent from the\\nnet for a while I figured I'd stick my neck out a bit...\\n\\nThe Royals will set the record for fewest runs scored by an AL\\nteam since the inception of the DH rule.  (p.s. any ideas what this is?)\\n\\nThey will fall easily short of 600 runs, that's for damn sure.  I can't\\nbelieve these media fools picking them to win the division (like our\\nTom Gage of the Detroit News claiming Herk Robinson is some kind of\\ngenius for the trades/aquisitions he's made)\\n\\nc-ya\\n\\nSean\\n\\n</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sorry for asking a question that's not entirely based on the\\ntechnical aspects of space, but I couldn't find the\\nanswer on the FAQs !\\n\\nI'm currently in the UK, which makes seeing a Space Shuttle\\nlaunch a little difficult.....\\n\\nHowever, I have been selected to be an exchange student\\nat Louisiana State Uni. from August, and I am absolutely\\ndetermined to get to see a Space Shuttle launch sometime\\nduring the year at which I will be in America.\\n\\nI hear there's a bit of a long mailing list, so if someone\\ncan tell me how to get tickets and where to get them from, then\\nplease E-mail me !\\n\\nThanks very much for your patience....\\n\\n(And if anyone else wants to know, tell me and I'll summarize\\nfor you - just to save all those poor people who have to\\npay for their links !)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Giant's have a five man rotation of  John Burkett, Trevor Wilson,\\nBill Swift, Jeff Brantley, and Bud Black/Dave Burba.  Black has\\nbeen put on the 15 day disables and Dave Burba will take his starts.\\n\\n</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\nLot's of these small miners  are no longer miners.  THey are people living\\nrent free on Federal land,  under the claim of being a miner.  The facts are\\nmany of these people do not sustaint heir income from mining,  do not\\noften even live their full time,  and do fotentimes do a fair bit\\nof environmental damage.\\n\\nThese minign statutes were created inthe 1830's-1870's  when the west was\\nuninhabited  and were designed to bring people into the frontier.  Times change\\npeople change.  DEAL.  you don't have a constitutional right to live off\\nthe same industry forever.  Anyone who claims the have a right to their\\njob in particular,  is spouting nonsense.   THis has been a long term\\nfederal welfare program,  that has outlived it's usefulness.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The term \"stopper\" is generally used to refer to a pitcher, one\\nwho can be counted on to pitch a strong game to keep his team from going\\non a losing streak.\\n\\n\\tThe Braves have plenty of pitchers to fit this description,\\nalthough right now I'd expect Smoltz or Glavine to take the mantle.\\n\\n\\tWhat the Braves lack, however, is an offensive stopper,\\nsomebody they can look to to bring them out of their hitting slump.\\nThere's just no one there.  The Braves got rid of their best pure\\nhitter, Lonnie Smith, and only Terry Pendleton on the current roster\\nhas ever shown more than a cursory ability to hit.\\t\\n\\n\\tOh, and another thing that worries me.  Ron Gant seems to have\\nslowed down a step.  That's scary.  A slow Ron Gant doesn't have much going\\nfor him.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16415 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      #  #/usr/bin/perl  \\\n",
       "I've been saying this for quite some time, but ...  0.0             0.0   \n",
       "Sorry for asking a question that's not entirely...  0.0             0.0   \n",
       "Giant's have a five man rotation of  John Burke...  0.0             0.0   \n",
       "\\n\\nLot's of these small miners  are no longer ...  0.0             0.0   \n",
       "The term \"stopper\" is generally used to refer t...  0.0             0.0   \n",
       "\n",
       "                                                    #0739   #1  #102  #14  \\\n",
       "I've been saying this for quite some time, but ...    0.0  0.0   0.0  0.0   \n",
       "Sorry for asking a question that's not entirely...    0.0  0.0   0.0  0.0   \n",
       "Giant's have a five man rotation of  John Burke...    0.0  0.0   0.0  0.0   \n",
       "\\n\\nLot's of these small miners  are no longer ...    0.0  0.0   0.0  0.0   \n",
       "The term \"stopper\" is generally used to refer t...    0.0  0.0   0.0  0.0   \n",
       "\n",
       "                                                    #150  #1506   #2  #2219  \\\n",
       "I've been saying this for quite some time, but ...   0.0    0.0  0.0    0.0   \n",
       "Sorry for asking a question that's not entirely...   0.0    0.0  0.0    0.0   \n",
       "Giant's have a five man rotation of  John Burke...   0.0    0.0  0.0    0.0   \n",
       "\\n\\nLot's of these small miners  are no longer ...   0.0    0.0  0.0    0.0   \n",
       "The term \"stopper\" is generally used to refer t...   0.0    0.0  0.0    0.0   \n",
       "\n",
       "                                                    ...  ~31  ~400   ~5  ~50  \\\n",
       "I've been saying this for quite some time, but ...  ...  0.0   0.0  0.0  0.0   \n",
       "Sorry for asking a question that's not entirely...  ...  0.0   0.0  0.0  0.0   \n",
       "Giant's have a five man rotation of  John Burke...  ...  0.0   0.0  0.0  0.0   \n",
       "\\n\\nLot's of these small miners  are no longer ...  ...  0.0   0.0  0.0  0.0   \n",
       "The term \"stopper\" is generally used to refer t...  ...  0.0   0.0  0.0  0.0   \n",
       "\n",
       "                                                    ~85  ~ftp/pub/rsdwg  \\\n",
       "I've been saying this for quite some time, but ...  0.0             0.0   \n",
       "Sorry for asking a question that's not entirely...  0.0             0.0   \n",
       "Giant's have a five man rotation of  John Burke...  0.0             0.0   \n",
       "\\n\\nLot's of these small miners  are no longer ...  0.0             0.0   \n",
       "The term \"stopper\" is generally used to refer t...  0.0             0.0   \n",
       "\n",
       "                                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "I've been saying this for quite some time, but ...                                                0.0                            \n",
       "Sorry for asking a question that's not entirely...                                                0.0                            \n",
       "Giant's have a five man rotation of  John Burke...                                                0.0                            \n",
       "\\n\\nLot's of these small miners  are no longer ...                                                0.0                            \n",
       "The term \"stopper\" is generally used to refer t...                                                0.0                            \n",
       "\n",
       "                                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \\\n",
       "I've been saying this for quite some time, but ...                                                0.0                              \n",
       "Sorry for asking a question that's not entirely...                                                0.0                              \n",
       "Giant's have a five man rotation of  John Burke...                                                0.0                              \n",
       "\\n\\nLot's of these small miners  are no longer ...                                                0.0                              \n",
       "The term \"stopper\" is generally used to refer t...                                                0.0                              \n",
       "\n",
       "                                                      º   ñ-  \n",
       "I've been saying this for quite some time, but ...  0.0  0.0  \n",
       "Sorry for asking a question that's not entirely...  0.0  0.0  \n",
       "Giant's have a five man rotation of  John Burke...  0.0  0.0  \n",
       "\\n\\nLot's of these small miners  are no longer ...  0.0  0.0  \n",
       "The term \"stopper\" is generally used to refer t...  0.0  0.0  \n",
       "\n",
       "[5 rows x 16415 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# 2️⃣ Convert text into Bag-of-Words representation\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocess_text,  # <--- Use our custom preprocess_text function as the tokenizer\n",
    "    # min_df=0.01,  # Ignore terms that have a document frequency strictly lower than the given threshold\n",
    "    # max_df=0.9,  # Ignore terms that have a document frequency strictly higher than the given threshold\n",
    ")\n",
    "\n",
    "\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)  # Learn the vocabulary and transform the data into feature vectors\n",
    "X_test = vectorizer.transform(newsgroups_test.data) # Transform the test data into feature vectors (Never fit your test data!)\n",
    "\n",
    "# Target labels (1=space 0=baseball)\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "# Create a DataFrame from the vectorizer vocabulary\n",
    "vocab = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out(), index=newsgroups_train.data)\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the TF-IDF scores are floating-point numbers, unlike the integer counts in the CountVectorizer. This is because the TF-IDF score is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ Train a Naive Bayes classifier (good for BoW text data)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4️⃣ Predict on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 0.9406\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.93      0.95      0.94       397\n",
      "         sci.space       0.95      0.93      0.94       394\n",
      "\n",
      "          accuracy                           0.94       791\n",
      "         macro avg       0.94      0.94      0.94       791\n",
      "      weighted avg       0.94      0.94      0.94       791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy by comparing predicted labels (y_pred) with actual labels (y_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate a detailed classification report (precision, recall, f1-score)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook drills down into a pretty old mechanism to represent text as numbers called TF-IDF a.k.a Term Frequency Inverse Document Frequency.\n",
    "\n",
    "So, what is TF-IDF? \n",
    "\n",
    "Intuitively, to understand what text is about, we look for words that occur frequently. Term frequency covers that aspect by capturing the number of times each word occurs in the text. To downgrade the relative importance of words that occur all too frequently, an inverse weighting is introduced to scale down the words that occur too frequently. This inverse weighting is referred to as Inverse Document Frequency. Together, TF-IDF captures the relative importance of words in a set of documents or a collection of texts.\n",
    "\n",
    "There are many great articles written on the intuition behind TF-IDF and not many written on how to derive the exact values for TF-IDF. The focus of this notebook is to piece together various calculations involved and provide how to derive each step programmatically so that you can derive it on the texts you are working with. We will look at the maths involved in an intuitive way as we go along. \n",
    "\n",
    "We will be using a beautiful poem by mystic poet Rumi as our example corpus. First, we will calculate TF IDF values for the poem using TF IDF Vectoriser from the sklearn package. Then, we will pull apart the various components and work through various steps involved in calculating TF-IDF values. Mathematical calculations and Python code will be provided for each step.\n",
    "\n",
    "Understanding TF-IDF calculations from scratch will help you with developing better intuitions on the results you obtain from applying any algorithm on TF-IDF values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Corpus<a name=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the concept of TF-IDF, we need a corpus. A corpus is a collection of documents. In a typical Natural Language Processing problem, a corpus can vary from a list of call center logs/ transcripts, a list of social media feedback to a large collection of research documents.\n",
    "\n",
    "To illustrate the various steps involved, we need to keep the corpus as small as possible. I chanced upon this quote / beautiful poem from the 13th century Persian Poet and Sufi Mystic Rumi (Jalāl ad-Dīn Muhammad Rūmī) and it fits our use case perfectly. So, we will be using this poem as our list of documents with each sentence considered as a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Rumi poem](RumiQuote.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =  [\"you were born with potential\",\n",
    "\"you were born with goodness and trust\",\n",
    "\"you were born with ideals and dreams\",\n",
    "\"you were born with greatness\",\n",
    "\"you were born with wings\",\n",
    "\"you are not meant for crawling, so don't\",\n",
    "\"you have wings\",\n",
    "\"learn to use them and fly\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking ahead to the final output<a name=\"output\"></a>\n",
    "We will be decimating the beautiful poem into mysterious decimals in this step. But, hey, after all, we are trying to demystify these decimals by understanding the  calculations involved in TF-IDF. As mentioned before, it is quite easy to derive through sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the tf idf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vect = TfidfVectorizer()\n",
    "X_train_tf_idf = tf_idf_vect.fit_transform(corpus)\n",
    "terms = tf_idf_vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "As the outputs are sparse matrices, for the ease of visualisation it is converted to dataframe. As the matrix is very small(8 rows * 25 columns), memory is not a constraint here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from a word matrix\n",
    "def dtm2df(wm, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)\n",
    "\n",
    "def idf2df(wm, feat_names):\n",
    "  \n",
    "    # create an index for each row\n",
    "    # doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm, index=[0],\n",
    "                      columns=feat_names)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf = dtm2df(X_train_tf_idf ,terms)\n",
    "display(HTML(df_tf_idf.to_html()))  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix above, each row represents a sentence from the above poem. Each column represents a unique word in the corpus above in alphabetical order. As you can see, there are lot of zeros in the matrix. So, a memory efficient sparse matrix is used for representing this. I have converted it to a data frame for ease of visualization.\n",
    "\n",
    "Let us interpret the numbers we have received so far. As you may have noticed, the words \"you were born\" are repeated throughout the corpus. So, we anticipate that these words will not be getting high TF-IDF scores. Manually if you look at the values for those three words, you can see that most often they get .2 and .3.\n",
    "\n",
    "Let us look at Document 0- You were born with potential. The word potential stands out. If you look at the various TF-IDF values in the first row in the matrix, you will see that the word potential has the highest TF-IDF value.\n",
    "Let us look at Document 4 (row 5): You were born with wings. Again, same as before, the word \"wings\" has the highest value in that sentence. \n",
    "\n",
    "Notice that the word \"wings\" appears also in Document 6. TF-IDF value for the word wings in Document 6 is different to TF-IDF value for the word wings in Document 4. In Document 6, the word \"wings\" is deemed less important than the word \"have\" in Document 6 as the word \"have\" appears only once in the entire corpus.\n",
    "\n",
    "The objective of this article is to look at how the above TF-IDF values can be calculated from scratch. We will be focusing on applying the calculations on the words wings and potential in particular, to derive the values highlighted in red in the matrix displayed above.\n",
    "\n",
    "We will break apart the various components and then put them back together. We will do this in three steps:\n",
    "* Step 1: Derive term frequency values \n",
    "* Step 2: Derive inverse document frequency values \n",
    "* Step 3: Aggregate the above two values using multiplication and normalization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Calculate Term Frequency<a name=\"tf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term frequency is pretty straight forward. It is calculated as the number of times the words/terms appear in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the following 2 documents\n",
    "\n",
    "* Document 0 - \"You were born with potential\" - each of the words appear once in this document\n",
    "* Document 4 - \"You were born with wings\" - again, each of the words appear once in this document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Count Vectorizer Example](.\\images\\count_vect_small.png \"count vectorizer example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use CountVectorizer to count the words and display the word count matrix. The matrix obtained below is also known as \"Bag Of Words\" or \"Document Term Matrix\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of times each time a word appears in a document (a sentence in the case of our corpus)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "terms = count_vect.get_feature_names()\n",
    "\n",
    "df_count = dtm2df(X_train_counts ,terms)\n",
    "display(HTML(df_count.to_html()))  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Count Vectorizer All](.\\images\\count_vect_all.png \"count vectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above: \n",
    "* Term Frequency of potential in Doc0 = 1\n",
    "* Term Frequency of wings in Doc5 = 1\n",
    "* Term Frequency of wings in Doc 7 = 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Calculating Inverse Document Frequency<a name=\"idf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the relative importance of frequently used words, we need a mechanism to tone down the importance of such words. Enter Inverse Document Frequency. Intuitively, if a word appears in all documents, then it may not play such a big part in differentiating between the documents. \n",
    "\n",
    "<i> Similar to Term Frequency </i>\n",
    "* <i>Document Frequency(term t) = number of documents with the term t/ total number of documents = d(t)/n</i>\n",
    "* <i>Inverse Document Frequency = total number of documents / number of documents with the term t = n / d(t)</i>\n",
    "\n",
    "Logarithmic scale intuitively makes sense to be used here as log(1) is 0. However, there are some practical considerations such as avoiding the division by 0 error, 1 is added to the denominator.\n",
    "Inverse Document frequency for the default settings in TF IDF vectorizer in sklearn is calculated as below (default settings have smooth_idf=True which adds 1 to numerator and denominator).\n",
    "n is the total number of documents in the document set.\n",
    "d(t) is the number of documents in the document set that contain term .\n",
    "\n",
    "$$ idf(t) = \\ln (\\frac{1+n}{1+df(t)})+1 $$\n",
    "* $n$  is the total number of documents in the document set, and \n",
    "* $d(t)$ is the number of documents in the document set that contain term . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the above formula for calidf value for the word potential\n",
    "* Number of Documents = 8\n",
    "* Number of documents in the corpus that contain the word 'potential' = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ idf(potential) = \\ln (\\frac{1+8}{1+1})+1\n",
    "                  = \\ln (\\frac {9}{2})+1\n",
    "                  = 2.504077 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the above formula for calidf value for the word potential\n",
    "* Number of Documents = 8\n",
    "* Number of documents in the corpus that contain the word 'wings' = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ idf(wings) = \\ln (\\frac{1+8}{2+1})+1\n",
    "                  = \\ln (\\frac {9}{3})+1\n",
    "                  = 2.098612 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore idf\n",
    "# idf_ attribute can be used to extract IDF values\n",
    "# transpose the 1D IDF array to convert to a dataframe to make it easy to visualise\n",
    "df_idf = idf2df(vectorizer.idf_[:,np.newaxis].T ,terms)\n",
    "display(HTML(df_idf.to_html()))    # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the values as shown below and we can cross-check the values from our calculations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Aggregate TF and IDF<a name=\"aggregate_tf_idf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name implies TF-IDF is a combination of Term Frequency(TF) and Inverse Document Frequency(IDF), obtained by multiplying the 2 values together. The sklearn implementation then applies normalization on the product between TF and IDF. Let us look at each of those steps in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3A:  Multiply TF and IDF<a name=\"multiply_tf_idf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiplying the 2 matrices together, we take an element-wise multiplication of Term Frequency Matrix and Inverse Document Frequency. Consider the first sentence - \"You were born with potential\". To find the product of TF and IDF for this sentence, it is calculated as below.![TF.IDF Example](.\\images\\tf.idf_example.png \"TF.IDF example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elment wise dot product\n",
    "df_mul = df_count.mul(df_idf.to_numpy())\n",
    "display(HTML(df_mul.to_html()))    # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the product of TF and IDF can be above 1. \n",
    "Now, the last step is to normalize these values so that TF-IDF values always scale between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 B: Normalize TF-IDF product<a name=\"normalize_tf_idf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In day to day life, when we want to normalize the values so that we can compare them easily we use percentages or proportions. We can potentially calculate a proportion of TF-IDF values across various words in a sentence. \n",
    "\n",
    "Note that both TF and IDF are non-negative values as the lowest value possible for Term Frequency and Inverse Document Frequency is 0. So, taking out proportions would be equivalent to what is known as L1 normalization. In L1 normalization each value in a vector(think various TF-IDF values of a sentence) is divided by sum of absolute values all elements. \n",
    "\n",
    "There is an option to L1 normalize the values in sklearn, but that is not the default setting.\n",
    "Default normalization applied is L2 normalization.Easiest way to think about L2 normalization is to think about the length of a line or Pythogoras theorem.\n",
    "![L2 Normalization](.\\images\\eg_L2_normalisation.png \"L2 Normalization example\")\n",
    "\n",
    "In the diagram above, the length of the line is 5. In this case, the line is a 1D vector. When vectors are n-dimensional, the length of the vector is similar to length of a line but extended to n dimensions. So, if a vector v is composed of n-elements, the length of the vector is calculated as\n",
    "![Length Vector](.\\images\\length_vector.png \"Length of a Vector\")\n",
    "\n",
    "In L2 normalization, we are essentially dividing the vector by the length of the vector . For a more mathematical explanation of L1 and L2 norm, please refer to Wikipedia.\n",
    "To apply L2 norm, for each of the sentences we need to calculate the square root of sum of squares of the product of TF and IDF.\n",
    "It can be done in Python as below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the values obtained\n",
    "![Euclidean Distance](.\\images\\euclidean_distance_tfidf.png \"Euclidean Distance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to calculate the final scores\n",
    "* <i>TF-IDF for the word potential in \"you were born with potential\" (Doc 0): 2.504077 / 3. 66856427 = 0.682895</i>\n",
    "* <i>TF-IDF for the word wings in you were born with wings Doc4 = 2.098612/ 3. 402882126 = 0.616716</i>\n",
    "* <i>TF-IDF for the word wings in Doc6 = 2.098612/ 3. 452116387 = 0.607744</i>\n",
    "\n",
    "This can be programatically achieved by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "df_mul.iloc[:,:] = Normalizer(norm='l2').fit_transform(df_mul)\n",
    "display(HTML(df_mul.to_html()))  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the above code, you get the results as below, which is same as the matrix in final output session\n",
    "So, far we have done the following\n",
    "![Calculation Overview](.\\images\\calculation_overview.png \"Calculation Overview\")\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why TF IDF is still relevant? <a name=\"relevance\"></a>\n",
    "In the field of Natural Language Processing, word embedding in 2013 and language models in 2018 have changed the landscape and led to many state of the art models for problems in NLP. So, it seems a bit strange that here in 2020, I have chosen to talk about TF IDF which was first formulated in 1970. Here are my 2 reasons why I think it is good to understand the underlying calculations.\n",
    "1. Understanding TF-IDF makes it one step easier to understand the results of algorithms you apply on top of TF-IDF. \n",
    "2. In 2018, Google released a text classification framework based on 450K experiments on a few different text sets. In text classification problems, the algorithms have to predict the topic based on a predefined set of topics it has trained on. Text classification problem is a common problem to solve for many companies. Based on the 450K experiments, Google found that when the number of samples/number of words < 1500, TF IDF was the best way to represent text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations <a name=\"limitations\"></a>\n",
    "The main limitation of TF IDF is that word order which is an important part of understanding the meaning of a sentence is not considered in TF-IDF.\n",
    "Another limitation is that document length can introduce a lot of variance in the TF IDF values. ??\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions and Conventions <a name=\"assumptions\"></a>\n",
    "Following are the assumptions and conventions used in this article.\n",
    "1. Rounding - For the ease of visualization, decimals are rounded to 4 decimal places. I have taken care not to round the numbers that are called out as examples. Any rounding changes you may spot in various cells are because of that. \n",
    "2. As mentioned in the sklearn documentation, there is a slight difference between most text-book formula for IDF and the implementation in sklearn.\n",
    "3. For simplicity, I have used default settings for sklearn TF IDF vectorizer. There are ways to alter this such as \n",
    "\n",
    "    * a) Use L1 normalization instead of L2 normalization\n",
    "    * b) Omit using smooth_idf in which case 1 that is added to numerator and denominator will be omitted. <divide by zero error> \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References: <a name=\"references\"></a>\n",
    "* [1] <a href = \"https://scikit-learn.org/stable/modules/feature_extraction.html\">scikit-learn documentation</a> \n",
    "* [2]<a href=\"https://developers.google.com/machine-learning/guides/text-classification/step-2-5\">Text Classification Framework</a>, Google, 2018 \n",
    "* [3]Link to the medium post for helper function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Notes <a name=\"notes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF IDF can also be calculated by transforming the CountVectorizer as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#transform the count vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf_idf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf_idf.shape\n",
    "print(dtm2df(X_train_tf_idf ,terms))    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
